# Modeling
## Natural Language Model
### 2017
- [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- [Sparse Gated MoE](https://arxiv.org/abs/1701.06538)
### 2019
- [Transformer-XL](https://arxiv.org/abs/1901.02860)
### 2020
- [Performers](https://arxiv.org/abs/2009.14794)
### 2023
- [LLaMA](https://arxiv.org/abs/2302.13971)
- [Mistral 7B](https://arxiv.org/abs/2310.06825)
### 2024
- [Mistral 8x7B](https://arxiv.org/abs/2401.04088)

## Automous Driving
### 2016
- [PointNet](https://arxiv.org/abs/1612.00593)
### 2019
- [MultiPath](https://arxiv.org/abs/1910.05449)
### 2020
- [VectorNet](https://arxiv.org/abs/2005.04259)
- [RSN](https://arxiv.org/abs/2005.09927)
### 2022
- [SWFormer](https://arxiv.org/abs/2210.07372)

# Infrastructure
## Optimization
### 2022
- [FlashAttention](https://arxiv.org/abs/2205.14135)
### 2023
- [FlashAttention-2](https://arxiv.org/abs/2307.08691)

## Quantization
### 2019
- [HAWQ](https://arxiv.org/abs/1905.03696)
- [HAWQ-V2](https://arxiv.org/abs/1911.03852)
### 2022
- [GPTQ](https://arxiv.org/abs/2210.17323)
- [FP8 Format](https://arxiv.org/abs/2209.05433)
### 2023
- [AWQ](https://arxiv.org/abs/2306.00978)
- [FP8-LM](https://arxiv.org/abs/2310.18313)

## Sparsity
### 2021
- [Nvidia Structured Sparsity](https://arxiv.org/abs/2104.08378)

## Distributed Training
### 2021
- [Sequence Parallelism](https://arxiv.org/abs/2105.13120)
