# Modeling
## Natural Language
### 2017
- [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- [Sparse Gated MoE](https://arxiv.org/abs/1701.06538)
### 2019
- [Transformer-XL](https://arxiv.org/abs/1901.02860)
### 2020
- [Performers](https://arxiv.org/abs/2009.14794)
- [Longformer(SlidingWindowAttention)](https://arxiv.org/abs/2004.05150)
- [GPT3](https://arxiv.org/abs/2005.14165)
### 2023
- [LLaMA](https://arxiv.org/abs/2302.13971)
- [Mistral 7B](https://arxiv.org/abs/2310.06825)
### 2024
- [Mistral 8x7B](https://arxiv.org/abs/2401.04088)
- [Mamba](https://arxiv.org/abs/2312.00752)

## Multimodality & Diffusion
## 2020
- [ViT](https://arxiv.org/abs/2010.11929)
## 2021
- [DiT](https://arxiv.org/abs/2112.10752)
- [SwinTransformer(ShiftedWindowAttention)](https://arxiv.org/abs/2103.14030)
## 2022
- [LDM](https://arxiv.org/abs/2212.09748)
- [eDiff-I](https://arxiv.org/abs/2211.01324)
## 2024
- [MM-DiT](https://arxiv.org/abs/2403.03206)

## Automous Driving
### 2016
- [PointNet](https://arxiv.org/abs/1612.00593)
### 2019
- [MultiPath](https://arxiv.org/abs/1910.05449)
### 2020
- [VectorNet](https://arxiv.org/abs/2005.04259)
- [RSN](https://arxiv.org/abs/2005.09927)
### 2022
- [SWFormer](https://arxiv.org/abs/2210.07372)

# Infrastructure
## Optimization
### 2022
- [Memory Efficient Attention](https://arxiv.org/abs/2112.05682)
- [FlashAttention](https://arxiv.org/abs/2205.14135)
### 2023
- [FlashAttention-2](https://arxiv.org/abs/2307.08691)

## Quantization
### 2019
- [HAWQ](https://arxiv.org/abs/1905.03696)
- [HAWQ-V2](https://arxiv.org/abs/1911.03852)
### 2022
- [GPTQ](https://arxiv.org/abs/2210.17323)
- [FP8 Format](https://arxiv.org/abs/2209.05433)
### 2023
- [AWQ](https://arxiv.org/abs/2306.00978)
- [FP8-LM](https://arxiv.org/abs/2310.18313)

## Sparsity
### 2021
- [Nvidia Structured Sparsity](https://arxiv.org/abs/2104.08378)

## Distributed Training
### 2021
- [Sequence Parallelism](https://arxiv.org/abs/2105.13120)
### 2023
- [Blockwise Parallelism](https://arxiv.org/abs/2305.19370)
- [Context Parallelism](https://arxiv.org/abs/2310.01889)
